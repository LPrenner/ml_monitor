{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed \n",
    "RSEED = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training Data\n",
    "\n",
    "train_data = pd.read_csv(\"../00_Data_Sets/1_mio_dataset_2010_2014.csv\");\n",
    "test_data = pd.read_csv(\"../00_Data_Sets/100_k_dataset_2015.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Input Data\n",
    "incoming_data_passenger_count_outlier = pd.read_csv(\"../01_Synthetic Data/02_Passenger Count/passenger count_outlier_01.csv\");\n",
    "# incoming_data_location_outlier = pd.read_csv(\"../01_Synthetic Data/01_Location/location_outlier_01.csv\");\n",
    "# incoming_data_distance_outlier = pd.read_csv(\"../01_Synthetic Data/03_Distance/distance_outlier_01.csv\");\n",
    "# incoming_data_distance_drift = pd.read_csv(\"../01_Synthetic Data/03_Distance/distance_drift_total_01.csv\");\n",
    "# incoming_data_location_drift = pd.read_csv(\"../01_Synthetic Data/01_Location/location_drift_total.csv\");\n",
    "incoming_data_passenger_drift = pd.read_csv(\"../01_Synthetic Data/02_Passenger Count/passenger count_drift_total_01.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = incoming_data_passenger_count_outlier.head(608)\n",
    "# df = df[df.groupby('fare-bin')['fare-bin'].transform(len) > 1]\n",
    "# df = prepare_incoming_batch(df, 0, 608);\n",
    "# df['fare-bin'].value_counts()\n",
    "# df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_drift_main_metrics(training_data, incoming_batch, start_index, end_index, batch_info, column, thresholds):\n",
    "    \n",
    "    batch_thresholds = thresholds[batch_info['name'].lower() + '_batch_drift']\n",
    "    threshold = batch_thresholds['one_dim_drift_metric']\n",
    "            \n",
    "    if abs(incoming_batch.iloc[start_index:end_index,column].mean()) > abs((training_data.iloc[:,column].mean() * (1 + threshold))):\n",
    "        print('[DRIFT][{} Window]: Upwards Data Drift detected!. ({} AVG: {})\\t(Index: {})'.format(batch_info['name'], training_data.columns[column], incoming_batch.iloc[start_index:end_index,column].mean(), end_index))\n",
    "        \n",
    "    if abs(incoming_batch.iloc[start_index:end_index,column].quantile(0.25)) > abs((training_data.iloc[:,column].quantile(0.25) * (1 + threshold))):\n",
    "        print('[DRIFT][{} Window]: Upwards Data Drift detected! ({} 25%-Quantile: {})\\t(Index: {})'.format(batch_info['name'], training_data.columns[column], incoming_batch.iloc[start_index:end_index,column].mean(), end_index))\n",
    "        \n",
    "    if abs(incoming_batch.iloc[start_index:end_index,column].quantile(0.75)) > abs((training_data.iloc[:,column].quantile(0.75) * (1 + threshold))):\n",
    "        print('[DRIFT][{} Window]: Upwards Data Drift detected! ({} 75%-Quantile: {})\\t(Index: {})'.format(batch_info['name'], training_data.columns[column], incoming_batch.iloc[start_index:end_index,column].mean(), end_index))\n",
    "        \n",
    "    if abs(incoming_batch.iloc[start_index:end_index,column].mean()) < abs((training_data.iloc[:,column].mean() * (1 - threshold))):\n",
    "        print('[DRIFT][{} Window]: Downwards Data Drift detected! ({} AVG: {})\\t(Index: {})'.format(batch_info['name'], training_data.columns[column], incoming_batch.iloc[start_index:end_index,column].mean(), end_index))\n",
    "        \n",
    "    if abs(incoming_batch.iloc[start_index:end_index,column].quantile(0.25)) < abs((training_data.iloc[:,column].quantile(0.25) * (1 - threshold))):\n",
    "        print('[DRIFT][{} Window]: Downwards Data Drift detected! ({} 25%-Quantile: {})\\t(Index: {})'.format(batch_info['name'], training_data.columns[column], incoming_batch.iloc[start_index:end_index,column].mean(), end_index))\n",
    "        \n",
    "    if abs(incoming_batch.iloc[start_index:end_index,column].quantile(0.75)) < abs((training_data.iloc[:,column].quantile(0.75) * (1 - threshold))):\n",
    "        print('[DRIFT][{} Window]: Downwards Data Drift detected! ({} 75%-Quantile: {})\\t(Index: {})'.format(batch_info['name'], training_data.columns[column], incoming_batch.iloc[start_index:end_index,column].mean(), end_index))\n",
    "\n",
    "        \n",
    "from math import log2\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    "\n",
    "        \n",
    "def monitor_drift_kl_divergence(training_data, incoming_batch, start_index, end_index, batch_info, column, thresholds):\n",
    "    \n",
    "    batch_thresholds = thresholds[batch_info['name'].lower() + '_batch_drift']\n",
    "    threshold = batch_thresholds['one_dim_drift_kl_divergence']\n",
    "    \n",
    "    # Filter out zero values in datasets\n",
    "    batch_df = incoming_batch[incoming_batch.iloc[:,column] != 0]\n",
    "    train_df = training_data[training_data.iloc[:,column] != 0]\n",
    "    \n",
    "    # Filter out negative values in dataset, if mean is positive (= remove negative outliers)\n",
    "    if batch_df.iloc[:,column].mean() > 0:\n",
    "        batch_df = batch_df[batch_df.iloc[:,column] > 0]\n",
    "        \n",
    "    batch_df = batch_df.reset_index()\n",
    "    train_df = train_df.reset_index()\n",
    "    batch_df = batch_df.drop('index', axis=1)\n",
    "    train_df = train_df.drop('index', axis=1)\n",
    "    batch = batch_df.iloc[start_index:end_index,column]\n",
    "    batch = batch.reset_index()\n",
    "    batch = batch.drop('index', axis=1)\n",
    "    batch = batch.iloc[:,0]\n",
    "    train = train_df.iloc[:,column]\n",
    "    \n",
    "    divergence_score = kl_divergence(batch, train);\n",
    "    \n",
    "    if divergence_score > threshold:\n",
    "        print('[DRIFT][{} Window]: KL-Divergence detected! ({} Divergence: {})\\t(Index: {})'.format(batch_info['name'], batch_df.columns[column], divergence_score, end_index))        \n",
    "        \n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "def monitor_drift_wasserstein_distance(training_data, incoming_batch, start_index, end_index, batch_info, column, thresholds):\n",
    "    \n",
    "    batch_thresholds = thresholds[batch_info['name'].lower() + '_batch_drift']\n",
    "    threshold = batch_thresholds['one_dim_drift_wasserstein']\n",
    "    \n",
    "    batch = incoming_batch.iloc[start_index:end_index, column]\n",
    "    train = training_data.iloc[:, column]\n",
    "    wasserstein_dist = wasserstein_distance(batch, train);\n",
    "    \n",
    "    if wasserstein_dist > threshold:\n",
    "        print('[DRIFT][{} Window]: Great Wasserstein Distance detected! ({} Distance: {})\\t(Index: {})'.format(batch_info['name'], incoming_batch.columns[column], wasserstein_dist, end_index))\n",
    "        \n",
    "def monitor_drift_pairwise_correlations(training_data, incoming_batch, start_index, end_index, batch_info, input_columns, train_pairwise_correlations, thresholds):\n",
    "    batch_thresholds = thresholds[batch_info['name'].lower() + '_batch_drift']\n",
    "    threshold = batch_thresholds['one_dim_drift_correlations']\n",
    "    \n",
    "    batch_pairwise_correlations = get_pairwise_correlations(incoming_batch.iloc[start_index:end_index,:], input_columns);\n",
    "    \n",
    "    for key in train_pairwise_correlations:\n",
    "        for metric in range(0,1):\n",
    "            distance = abs((train_pairwise_correlations[key][metric] - batch_pairwise_correlations[key][metric]))\n",
    "            if distance > abs((train_pairwise_correlations[key][metric] * threshold)):\n",
    "                print('[DRIFT][{} Window]: Pairwise Correlation Drift detected: {}!\\t(Index: {})'.format(batch_info['name'], key, end_index))\n",
    "\n",
    "    \n",
    "def prepare_incoming_batch(data, start_index, end_index):\n",
    "    # Remove fare-bin values with less occurences than 3; Avoid error in split data set\n",
    "    df = data.iloc[start_index:end_index,:]\n",
    "    df = df[df.groupby('fare-bin')['fare-bin'].transform(len) > 3]\n",
    "    return df\n",
    "    \n",
    "def monitor_feature_importances(training_data, incoming_batch, start_index, end_index, batch_info, input_columns, feature_importances, thresholds):\n",
    "    batch_thresholds = thresholds[batch_info['name'].lower() + '_batch_drift']\n",
    "    threshold = batch_thresholds['one_dim_drift_feature_importances']\n",
    "    \n",
    "    incoming_batch = prepare_incoming_batch(incoming_batch, start_index, end_index);\n",
    "  \n",
    "    rf = get_rf(incoming_batch, input_columns);\n",
    "    batch_feature_importance_list = rf.feature_importances_\n",
    "    train_feature_importance_list = feature_importances\n",
    "    feature_count = len(batch_feature_importance_list)\n",
    "    \n",
    "    for index in range(0, feature_count):\n",
    "        if batch_feature_importance_list[index] > (train_feature_importance_list[index] * (1 + threshold)):\n",
    "            print('[DRIFT][{} Window]: Feature Importance Drift detected for {}!\\t(Index: {})'.format(batch_info['name'], incoming_batch.columns[input_columns[index]], end_index))\n",
    "            \n",
    "        if batch_feature_importance_list[index] < (train_feature_importance_list[index] / (1 + threshold)):\n",
    "            print('[DRIFT][{} Window]: Feature Importance Drift detected for {}!\\t(Index: {})'.format(batch_info['name'], incoming_batch.columns[input_columns[index]], end_index))\n",
    "\n",
    "def monitor_drift_shap(train_data_infos, incoming_batch, start_index, end_index, batch_info, column, shap_index, thresholds):\n",
    "    batch_thresholds = thresholds[batch_info['name'].lower() + '_batch_drift']\n",
    "    threshold = batch_thresholds['one_dim_drift_shap']\n",
    "    shap = get_shap_feature_importances(incoming_batch.iloc[start_index:end_index,:], train_data_infos['rf'], train_data_infos['input_columns'], thresholds)\n",
    "\n",
    "    # SHAP Mean\n",
    "    if shap[1][shap_index] < (train_data_infos['shap']['mean'][shap_index] * (1 - threshold)):\n",
    "        print('[DRIFT][{} Window]: Downwards Shap Drift detected for {}!\\t(Index: {})'.format(batch_info['name'], train_data.columns[column], end_index));    \n",
    "        \n",
    "    # SHAP Mean\n",
    "    if shap[1][shap_index] > (train_data_infos['shap']['mean'][shap_index] * (1 + threshold)):\n",
    "        print('[DRIFT][{} Window]: Upwards Shap Drift detected for {}!\\t(Index: {})'.format(batch_info['name'], train_data.columns[column], end_index));            \n",
    "\n",
    "def monitor_drift_one_dimension(training_data, train_data_infos, incoming_batch, start_index, end_index, batch_info, thresholds):\n",
    "    input_columns = train_data_infos['input_columns']\n",
    "    output_columns = train_data_infos['output_columns']\n",
    "    columns_to_use = np.concatenate((output_columns, input_columns), axis=0)\n",
    "    train_pairwise_correlations = train_data_infos['correlation_pairs']\n",
    "    shap_index = 0\n",
    "    \n",
    "    for column in columns_to_use:\n",
    "        monitor_drift_main_metrics(training_data, incoming_batch, start_index, end_index, batch_info, column, thresholds);\n",
    "        monitor_drift_kl_divergence(training_data, incoming_batch, start_index, end_index, batch_info, column, thresholds);\n",
    "        monitor_drift_wasserstein_distance(training_data, incoming_batch, start_index, end_index, batch_info, column, thresholds);\n",
    "        # +++ SHAP IS COMMENTED OUT FOR PERFORMANCE REASONS +++\n",
    "        # if column in input_columns:\n",
    "            # monitor_drift_shap(train_data_infos, incoming_batch, start_index, end_index, batch_info, column, shap_index, thresholds);\n",
    "            # shap_index = shap_index + 1\n",
    "        \n",
    "    monitor_drift_pairwise_correlations(training_data, incoming_batch, start_index, end_index, batch_info, input_columns, train_pairwise_correlations, thresholds);\n",
    "    monitor_feature_importances(training_data, incoming_batch, start_index, end_index, batch_info, input_columns, train_data_infos['rf_feature_importances'], thresholds);\n",
    "    \n",
    "def monitor_drift_multi_dimensions(training_data, train_data_infos, incoming_batch, start_index, end_index, batch_info, thresholds):\n",
    "    input_columns = train_data_infos['input_columns'];\n",
    "    output_columns = train_data_infos['output_columns'];\n",
    "    pca_model = train_data_infos['pca_model'];\n",
    "    \n",
    "    incoming_batch_filtered = get_data_filtered(incoming_batch, input_columns);\n",
    "    incoming_batch_transformed = transform_pca(pca_model, incoming_batch_filtered);\n",
    "    distances = calculate_euclidean_distances(incoming_batch_transformed, input_columns);\n",
    "    distances_metrics = get_distance_metrics(distances, thresholds);\n",
    "    \n",
    "    if distances_metrics['mean'] > train_data_infos['distance_metrics']['mean']:\n",
    "        print('[DRIFT][{} Window]: Multi-Dimensional Drift Detected! Mean Distance: {}\\t\\t\\t(Index: {})'.format(batch_info['name'], distances_metrics['maximum'], end_index));\n",
    "        \n",
    "    if distances_metrics['first_quarter'] > train_data_infos['distance_metrics']['first_quarter']:\n",
    "        print('[DRIFT][{} Window]: Multi-Dimensional Drift Detected! Distance: {}\\t\\t\\t(Index: {})'.format(batch_info['name'], distances_metrics['first_quarter'], end_index));\n",
    "        \n",
    "    if distances_metrics['third_quarter'] > train_data_infos['distance_metrics']['third_quarter']:\n",
    "        print('[DRIFT][{} Window]: Multi-Dimensional Drift Detected! Distance: {}\\t\\t\\t(Index: {})'.format(batch_info['name'], distances_metrics['third_quarter'], end_index));\n",
    "        \n",
    "        \n",
    "def monitor_batch_drift(training_data, train_data_infos, incoming_batch, index, batch_info, thresholds):\n",
    "    batch_size = batch_info['size'];\n",
    "    start_index = index - batch_size + 1;\n",
    "    end_index = index;\n",
    "    \n",
    "    monitor_drift_one_dimension(training_data, train_data_infos, incoming_batch, start_index, end_index, batch_info, thresholds);\n",
    "    monitor_drift_multi_dimensions(training_data, train_data_infos, incoming_batch, start_index, end_index, batch_info, thresholds);\n",
    "    \n",
    "def monitor_drift(training_data, train_data_infos, batch, step_sizes, index, row, batch_infos, thresholds):\n",
    "    \n",
    "    # Add new sample to batch\n",
    "    batch.loc[index] = row.values;\n",
    "        \n",
    "    if batch.shape[0] % step_sizes['small'] == 0 and batch.shape[0] > (step_sizes['small']):\n",
    "        monitor_batch_drift(training_data, train_data_infos, batch, index, batch_infos['small'], thresholds);\n",
    "\n",
    "    if batch.shape[0] % step_sizes['medium'] == 0:\n",
    "        monitor_batch_drift(training_data, train_data_infos, batch, index, batch_infos['medium'], thresholds);\n",
    "\n",
    "    if batch.shape[0] % step_sizes['large'] == 0:\n",
    "        monitor_batch_drift(training_data, train_data_infos, batch, index, batch_infos['large'], thresholds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rf(data, columns):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(data, np.array(data['fare_amount']), \n",
    "                                                          stratify = data['fare-bin'], test_size=0.33,\n",
    "                                                          random_state = RSEED)\n",
    "\n",
    "    # Create the random forest        \n",
    "    rf = RandomForestRegressor(n_estimators = 20, max_depth = 20, max_features = None, oob_score = True, \n",
    "                                bootstrap = True, verbose = 1, n_jobs = -1)\n",
    "\n",
    "    # Train random forest\n",
    "    column_list = []\n",
    "    for column in columns:\n",
    "        column_list.append(train_data.columns[column])\n",
    "        \n",
    "    rf.fit(X_train[column_list], y_train)\n",
    "    \n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE RF\n",
    "\n",
    "# from sklearn.externals import joblib\n",
    "\n",
    "# input_columns = [3, 4, 5, 6, 7, 12, 13, 14]\n",
    "# rf = get_rf(train_data, input_columns)\n",
    "\n",
    "# filename = 'rf.sav'\n",
    "# joblib.dump(rf, filename);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_samples_per_day(data):\n",
    "    # TODO: More elegant solution possible?\n",
    "    df = pd.DataFrame();\n",
    "    df['pickup_datetime'] = train_data['pickup_datetime']\n",
    "    df['count'] = 1\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df.index = df['pickup_datetime'] \n",
    "    df = df.resample('D').sum()\n",
    "    df = df[df['count'] != 0]\n",
    "    return df['count'].mean()\n",
    "\n",
    "def get_batch_infos(data):\n",
    "    batch_size_day = get_avg_samples_per_day(data);\n",
    "    batch_size_week = batch_size_day * 7;\n",
    "    batch_size_month = batch_size_day * 30;\n",
    "    \n",
    "    batch_sizes = {\n",
    "        'small': {'size': int(batch_size_day), 'name': 'Small'},\n",
    "        'medium': {'size': int(batch_size_week), 'name': 'Medium'},\n",
    "        'large': {'size': int(batch_size_month), 'name': 'Large'},        \n",
    "    }\n",
    "    \n",
    "    return batch_sizes;\n",
    "\n",
    "def get_step_sizes(batch_infos):\n",
    "    \n",
    "    step_sizes = {\n",
    "        'small': (batch_infos['small']['size'] / 2),\n",
    "        'medium': (batch_infos['medium']['size'] / 2),\n",
    "        'large': (batch_infos['large']['size'] / 2) \n",
    "    }\n",
    "    return step_sizes;\n",
    "\n",
    "def get_data_filtered(data, columns_to_use):\n",
    "    data_filtered = pd.DataFrame()\n",
    "    i = 0\n",
    "\n",
    "    # Filter Input Columns\n",
    "    for column in columns_to_use:\n",
    "        data_filtered.insert(i, data.columns[column], data.iloc[:,column])\n",
    "        i = i + 1\n",
    "        \n",
    "    return data_filtered;\n",
    "\n",
    "def apply_pca(train_data_filtered):\n",
    "    pca_model = get_pca_model(train_data_filtered);\n",
    "    return pca_model, transform_pca(pca_model, train_data_filtered);\n",
    "\n",
    "def get_pca_model(train_data_filtered):\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(whiten=True)\n",
    "    return pca.fit(train_data_filtered)\n",
    "\n",
    "def transform_pca(pca_model, train_data_filtered):\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    return pca_model.transform(train_data_filtered)\n",
    "\n",
    "def get_zero_vector(columns_to_use):\n",
    "    number_of_dimensions = len(columns_to_use)\n",
    "    return np.zeros((number_of_dimensions)*1)\n",
    "\n",
    "def calculate_euclidean_distance(sample, zero_vector):\n",
    "    from scipy.spatial import distance\n",
    "    \n",
    "    return distance.euclidean(sample, zero_vector)\n",
    "\n",
    "def calculate_euclidean_distances(transformed_data, columns_to_use):\n",
    "    from scipy.spatial import distance\n",
    "    \n",
    "    zero_vector = get_zero_vector(columns_to_use);\n",
    "    \n",
    "    index = 0\n",
    "    distances = np.ndarray(shape=(np.size(transformed_data,0),1))\n",
    "\n",
    "    # Create distances to zero vector\n",
    "    for sample in transformed_data:\n",
    "        distances[index] = calculate_euclidean_distance(transformed_data[index], zero_vector);\n",
    "        index = index + 1\n",
    "        \n",
    "    return distances;\n",
    "\n",
    "def get_pairwise_correlations(data, input_columns):\n",
    "    \n",
    "    from scipy.stats.stats import pearsonr\n",
    "    import itertools\n",
    "    \n",
    "    pairwise_correlations = {}\n",
    "\n",
    "    for column_a, column_b in itertools.combinations(input_columns, 2):\n",
    "        correlation = pearsonr(data.iloc[:,column_a], data.iloc[:,column_b])\n",
    "        pairwise_correlations['{} <> {}'.format(data.columns[column_a], data.columns[column_b])] = correlation\n",
    "        \n",
    "    return pairwise_correlations\n",
    "\n",
    "\n",
    "def get_shap_feature_importances(data, rf, input_columns, thresholds):\n",
    "    import shap\n",
    "    raw_threshold = thresholds['one_dim_outlier']\n",
    "    threshold =     raw_threshold * 1000\n",
    "\n",
    "    column_list = []\n",
    "    for column in input_columns:\n",
    "        column_list.append(train_data.columns[column])\n",
    "\n",
    "    explainer = shap.TreeExplainer(rf);\n",
    "    shap_values = explainer.shap_values(data[column_list]);\n",
    "\n",
    "    shap_values_lower_quantile = []\n",
    "    shap_values_mean = []\n",
    "    shap_values_upper_quantile = []\n",
    "\n",
    "    for column in range(0, len(input_columns)):    \n",
    "        shap_values_lower_quantile.append(np.percentile(shap_values[:,column], threshold))\n",
    "        shap_values_mean.append(shap_values[:,column].mean())\n",
    "        shap_values_upper_quantile.append(np.percentile(shap_values[:,column], (100 - threshold)))\n",
    "        \n",
    "    shap = [shap_values_lower_quantile, shap_values_mean, shap_values_upper_quantile]\n",
    "    return shap\n",
    "    \n",
    "\n",
    "def get_distance_metrics(distances, thresholds):\n",
    "    \n",
    "    raw_threshold = thresholds['multi_dim_outlier']\n",
    "    threshold = raw_threshold * 100\n",
    "    \n",
    "    distance_metrics = {\n",
    "        'minimum': distances.min(),\n",
    "        'lower_threshold_percentile': np.percentile(distances, threshold),\n",
    "        'first_quarter': np.percentile(distances, 25),\n",
    "        'mean': distances.mean(),\n",
    "        'third_quarter': np.percentile(distances, 75),\n",
    "        'upper_threshold_percentile': np.percentile(distances, (100 - threshold)),\n",
    "        'maximum': distances.max()\n",
    "    }\n",
    "        \n",
    "    return distance_metrics\n",
    "    \n",
    "\n",
    "def get_train_data_infos(distances, correlation_pairs, shap, rf, input_columns, output_columns, pca_model, thresholds):\n",
    "    \n",
    "    train_data_infos = {\n",
    "        'input_columns': input_columns,\n",
    "        'output_columns': output_columns,\n",
    "        'pca_model': pca_model,\n",
    "        'rf': rf,\n",
    "        'rf_feature_importances': rf.feature_importances_,\n",
    "        'shap': {\n",
    "            'lower_threshold_percentile': shap[0],\n",
    "            'mean': shap[1],\n",
    "            'upper_threshold_percentile': shap[2]\n",
    "        },\n",
    "        'correlation_pairs': correlation_pairs,\n",
    "        'distance_metrics': get_distance_metrics(distances, thresholds)\n",
    "    }\n",
    "    \n",
    "    return train_data_infos;\n",
    "\n",
    "def generate_train_data_infos(train_data, input_columns, output_columns, thresholds):\n",
    "    # +++ REMOVE: Only for testing purposes +++\n",
    "    from sklearn.externals import joblib\n",
    "    rf = joblib.load('rf.sav')\n",
    "    test = {\n",
    "        'input_columns': input_columns,\n",
    "        'output_columns': output_columns,\n",
    "        'pca_model': get_pca_model(get_data_filtered(train_data, input_columns)),\n",
    "        'rf': rf,\n",
    "        'rf_feature_importances': rf.feature_importances_,\n",
    "        'shap': {\n",
    "            'lower_threshold_percentile': [-0.9627528245137031, -0.8588506007810717, -1.0228424799213929, -2.733344036928182, -0.21071108860192908, -0.7614152862846567, -7.004810823869412, -0.9397471843701778],\n",
    "            'mean': [0.05859532547100909, 0.1150723753541944, 0.2375574423112044, -0.21863576089415065, 0.0049141129696212635, 0.18305498308684381, -0.6148760801903526, 0.16000324432733556],\n",
    "            'upper_threshold_percentile': [2.984288788547222, 1.204538253875003, 4.797235571183871, 2.647309409210014, 0.2712167705821321, 1.3335546007529564, 36.2736142158481, 2.2940476694834837]\n",
    "        },\n",
    "        'correlation_pairs': get_pairwise_correlations(train_data, input_columns),\n",
    "        'distance_metrics': {\n",
    "            'minimum': 0.7249817600534729, \n",
    "            'lower_threshold_percentile': 1.0764902427267176,\n",
    "            'first_quarter': 1.1133579226848411,\n",
    "            'mean': 1.922411839793231,\n",
    "            'third_quarter': 2.200820905397247,\n",
    "            'upper_threshold_percentile': 8.095414750208105, \n",
    "            'maximum': 58.320576749831\n",
    "        }\n",
    "    }\n",
    "    return test\n",
    "    # +++ REMOVE: Only for testing purposes +++\n",
    "\n",
    "    train_data_filtered = get_data_filtered(train_data, input_columns);\n",
    "    pca_model, transformed_data = apply_pca(train_data_filtered);\n",
    "    distances = calculate_euclidean_distances(transformed_data, input_columns);\n",
    "    correlation_pairs = get_pairwise_correlations(train_data, input_columns);\n",
    "    rf = get_rf(train_data, input_columns);\n",
    "    shap = get_shap_feature_importances(train_data, rf, input_columns, thresholds);\n",
    "    \n",
    "    return get_train_data_infos(distances, correlation_pairs, shap, rf, input_columns, output_columns, pca_model, thresholds);  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_outliers_one_dimension(train_data, train_data_infos, incoming_sample, index, thresholds):\n",
    "    \n",
    "    threshold = thresholds['one_dim_outlier']\n",
    "    input_columns = train_data_infos['input_columns']\n",
    "    output_columns = train_data_infos['output_columns']\n",
    "    columns_to_use = np.concatenate((output_columns, input_columns), axis=0) \n",
    "    shap = get_shap_feature_importances(pd.DataFrame(incoming_sample).transpose(), train_data_infos['rf'], input_columns, thresholds)\n",
    "    shap_index = 0\n",
    "    \n",
    "    for column in columns_to_use:\n",
    "            \n",
    "            if incoming_sample[column] > train_data.iloc[:,column].max():\n",
    "                print('[OUTLIER] MAX Outlier Detected! {}: {}\\t\\t\\t(Index: {})'.format(train_data.columns[column], incoming_sample[column], index));\n",
    "\n",
    "            elif incoming_sample[column] > train_data.iloc[:,column].quantile(1 - threshold):\n",
    "                print('[POT. OUTLIER] Potential MAX Outlier Detected! {}: {}\\t\\t\\t(Index: {})'.format(train_data.columns[column], incoming_sample[column], index));\n",
    "\n",
    "            if incoming_sample[column] < train_data.iloc[:,column].min():\n",
    "                print('[OUTLIER] MIN Outlier Detected! {}: {}\\t\\t\\t(Index: {})'.format(train_data.columns[column], incoming_sample[column], index));\n",
    "                \n",
    "            elif incoming_sample[column] < train_data.iloc[:,column].quantile(threshold):\n",
    "                print('[POT. OUTLIER] Potential MIN Outlier Detected! {}: {}\\t\\t\\t(Index: {})'.format(train_data.columns[column], incoming_sample[column], index));\n",
    "                \n",
    "            if column in input_columns:\n",
    "            \n",
    "                # SHAP Lower Percentile\n",
    "                if shap[0][shap_index] < train_data_infos['shap']['lower_threshold_percentile'][shap_index]:\n",
    "                    print('[POT. MIN OUTLIER] Potential Shap Outlier Detected! {}\\t\\t\\t(Index: {})'.format(train_data.columns[column], index));\n",
    "\n",
    "                # SHAP Upper Percentile\n",
    "                if shap[2][shap_index] > train_data_infos['shap']['upper_threshold_percentile'][shap_index]:\n",
    "                    print('[POT. MAX OUTLIER] Potential Shap Outlier Detected! {}\\t\\t\\t(Index: {})'.format(train_data.columns[column], index));\n",
    "\n",
    "                shap_index = shap_index + 1\n",
    "\n",
    "    \n",
    "def monitor_outliers_multi_dimensions(train_data, train_data_infos, incoming_sample, index):\n",
    "    incoming_sample_df = pd.DataFrame(incoming_sample).transpose();\n",
    "    incoming_sample_df_filtered = get_data_filtered(incoming_sample_df, train_data_infos['input_columns']);\n",
    "    incoming_sample_df_transformed = transform_pca(train_data_infos['pca_model'], incoming_sample_df_filtered);\n",
    "    distance = calculate_euclidean_distance(incoming_sample_df_transformed, get_zero_vector(train_data_infos['input_columns']));\n",
    "\n",
    "    if distance > train_data_infos['distance_metrics']['maximum']:\n",
    "        print('[OUTLIER] Multi-Dimensional Outlier Detected! Distance: {}\\t\\t\\t(Index: {})'.format(distance, index));\n",
    "        \n",
    "    elif distance > train_data_infos['distance_metrics']['upper_threshold_percentile']:\n",
    "        print('[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: {}\\t\\t\\t(Index: {})'.format(distance, index));\n",
    "\n",
    "def monitor_outliers(train_data, train_data_infos, incoming_sample, index, thresholds):\n",
    "    monitor_outliers_one_dimension(train_data, train_data_infos, incoming_sample, index, thresholds);\n",
    "    monitor_outliers_multi_dimensions(train_data, train_data_infos, incoming_sample, index);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define desired thresholds\n",
    "\n",
    "def get_thresholds():\n",
    "    thresholds = {\n",
    "        'one_dim_outlier': 0.001,\n",
    "        'multi_dim_outlier': 0.1,\n",
    "        'small_batch_drift': {\n",
    "            'one_dim_drift_metric': 0.2,\n",
    "            'one_dim_drift_kl_divergence': 6000,\n",
    "            'one_dim_drift_wasserstein': 1.2,\n",
    "            'one_dim_drift_correlations': 50,\n",
    "            'one_dim_drift_feature_importances': 25,\n",
    "            'one_dim_drift_shap': 5\n",
    "        },\n",
    "        'medium_batch_drift': {\n",
    "            'one_dim_drift_metric': 0.1,\n",
    "            'one_dim_drift_kl_divergence': 1000,\n",
    "            'one_dim_drift_wasserstein': 0.25,\n",
    "            'one_dim_drift_correlations': 20,\n",
    "            'one_dim_drift_feature_importances': 20,\n",
    "            'one_dim_drift_shap': 3            \n",
    "        },\n",
    "        'large_batch_drift': {\n",
    "            'one_dim_drift_metric': 0.05,\n",
    "            'one_dim_drift_kl_divergence': 500,\n",
    "            'one_dim_drift_wasserstein': 0.2,\n",
    "            'one_dim_drift_correlations': 15,\n",
    "            'one_dim_drift_feature_importances': 15,\n",
    "            'one_dim_drift_shap': 2           \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return thresholds;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to be configured individually\n",
    "def prepare_data(data):\n",
    "    prepared_data = data.copy()\n",
    "    prepared_data['fare-bin'] = data.iloc[:,8].astype(\"category\").cat.codes\n",
    "    \n",
    "    return prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor(train_data, incoming_data):\n",
    "        \n",
    "    train_data = prepare_data(train_data);\n",
    "    incoming_data = prepare_data(incoming_data);\n",
    "    thresholds = get_thresholds();\n",
    "    input_columns = [3, 4, 5, 6, 7, 12, 13, 14]\n",
    "    output_columns = [1, 8]\n",
    "    batch = pd.DataFrame(columns=incoming_data.columns);\n",
    "    batch_infos = get_batch_infos(train_data);\n",
    "    train_data_infos = generate_train_data_infos(train_data, input_columns, output_columns, thresholds);\n",
    "    step_sizes = get_step_sizes(batch_infos);\n",
    "    \n",
    "    for index, row in incoming_data.iterrows():\n",
    "        monitor_outliers(train_data, train_data_infos, row, index, thresholds);\n",
    "        monitor_drift(train_data, train_data_infos, batch, step_sizes, index, row, batch_infos, thresholds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[POT. OUTLIER] Potential MAX Outlier Detected! dropoff_latitude: 40.887882232666016\t\t\t(Index: 4)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! manhattan\t\t\t(Index: 27)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! manhattan\t\t\t(Index: 67)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! passenger_count\t\t\t(Index: 89)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! dropoff_longitude\t\t\t(Index: 115)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! dropoff_latitude\t\t\t(Index: 115)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! dropoff_longitude\t\t\t(Index: 120)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! manhattan\t\t\t(Index: 120)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! haversine\t\t\t(Index: 132)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! dropoff_longitude\t\t\t(Index: 150)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! pickup_latitude\t\t\t(Index: 154)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! manhattan\t\t\t(Index: 154)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 8.604924725313644\t\t\t(Index: 154)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 9.120483539822029\t\t\t(Index: 161)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! dropoff_longitude\t\t\t(Index: 166)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! pickup_latitude\t\t\t(Index: 172)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! haversine\t\t\t(Index: 188)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! euclidean\t\t\t(Index: 239)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! dropoff_longitude\t\t\t(Index: 244)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! euclidean\t\t\t(Index: 244)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 8.305609213727125\t\t\t(Index: 244)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 8.136353255495477\t\t\t(Index: 277)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 8.725600535059865\t\t\t(Index: 287)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! haversine\t\t\t(Index: 305)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! euclidean\t\t\t(Index: 322)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 8.313814949308114\t\t\t(Index: 322)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! dropoff_longitude\t\t\t(Index: 343)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! manhattan: 0.36305999755859375\t\t\t(Index: 343)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! euclidean: 0.2629007645951818\t\t\t(Index: 343)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! euclidean\t\t\t(Index: 343)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! haversine: 27.40050231658701\t\t\t(Index: 343)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! haversine\t\t\t(Index: 343)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 12.049343616294522\t\t\t(Index: 343)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! dropoff_longitude\t\t\t(Index: 344)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! euclidean\t\t\t(Index: 344)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 10.145906069419583\t\t\t(Index: 344)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! pickup_longitude\t\t\t(Index: 379)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! dropoff_longitude\t\t\t(Index: 386)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! euclidean\t\t\t(Index: 386)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 8.849179660232366\t\t\t(Index: 386)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! manhattan\t\t\t(Index: 388)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! dropoff_longitude\t\t\t(Index: 389)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! pickup_latitude: 40.86755752563477\t\t\t(Index: 424)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! manhattan: 0.3475189208984233\t\t\t(Index: 440)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! euclidean: 0.2458510235458707\t\t\t(Index: 440)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! euclidean\t\t\t(Index: 440)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! haversine: 24.076034903779558\t\t\t(Index: 440)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! haversine\t\t\t(Index: 440)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 9.791922406193613\t\t\t(Index: 440)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! euclidean\t\t\t(Index: 444)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 9.677755204164434\t\t\t(Index: 444)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! euclidean\t\t\t(Index: 471)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 8.47425697092458\t\t\t(Index: 471)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! euclidean\t\t\t(Index: 528)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! euclidean\t\t\t(Index: 545)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 8.320671015454668\t\t\t(Index: 545)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! pickup_latitude\t\t\t(Index: 581)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 9.879007906230157\t\t\t(Index: 581)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 9.119399016972757\t\t\t(Index: 584)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DRIFT][Small Window]: Multi-Dimensional Drift Detected! Mean Distance: 12.04934361629452\t\t\t(Index: 607)\n",
      "[DRIFT][Small Window]: Multi-Dimensional Drift Detected! Distance: 1.2600161001278232\t\t\t(Index: 607)\n",
      "[DRIFT][Small Window]: Multi-Dimensional Drift Detected! Distance: 2.5070246716793303\t\t\t(Index: 607)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 11.937218647779211\t\t\t(Index: 621)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! dropoff_longitude: -73.62210845947266\t\t\t(Index: 631)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! dropoff_longitude\t\t\t(Index: 631)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! dropoff_latitude: 41.02389526367188\t\t\t(Index: 631)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! dropoff_latitude\t\t\t(Index: 631)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! manhattan: 0.495525360107429\t\t\t(Index: 631)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! euclidean: 0.3504990597107201\t\t\t(Index: 631)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! euclidean\t\t\t(Index: 631)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! haversine: 34.81891090248362\t\t\t(Index: 631)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! haversine\t\t\t(Index: 631)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 16.130677218461617\t\t\t(Index: 631)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! euclidean\t\t\t(Index: 649)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 8.378975709806522\t\t\t(Index: 649)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 9.335628450471807\t\t\t(Index: 666)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! dropoff_latitude\t\t\t(Index: 686)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 8.437535294336001\t\t\t(Index: 692)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! manhattan\t\t\t(Index: 701)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 9.123494441224077\t\t\t(Index: 701)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! pickup_longitude\t\t\t(Index: 750)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! pickup_latitude\t\t\t(Index: 750)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! manhattan\t\t\t(Index: 750)\n",
      "[POT. MAX OUTLIER] Potential Shap Outlier Detected! haversine\t\t\t(Index: 750)\n",
      "[POT. MIN OUTLIER] Potential Shap Outlier Detected! dropoff_longitude\t\t\t(Index: 752)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1d63bec9d0c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-ba0b55b030d7>\u001b[0m in \u001b[0;36mmonitor\u001b[0;34m(train_data, incoming_data)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mincoming_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mmonitor_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mmonitor_drift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-830c65c69f00>\u001b[0m in \u001b[0;36mmonitor_outliers\u001b[0;34m(train_data, train_data_infos, incoming_sample, index, thresholds)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmonitor_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincoming_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mmonitor_outliers_one_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincoming_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mmonitor_outliers_multi_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincoming_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-830c65c69f00>\u001b[0m in \u001b[0;36mmonitor_outliers_one_dimension\u001b[0;34m(train_data, train_data_infos, incoming_sample, index, thresholds)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[OUTLIER] MIN Outlier Detected! {}: {}\\t\\t\\t(Index: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincoming_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mincoming_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[POT. OUTLIER] Potential MIN Outlier Detected! {}: {}\\t\\t\\t(Index: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincoming_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mquantile\u001b[0;34m(self, q, interpolation)\u001b[0m\n\u001b[1;32m   2400\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2402\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2404\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mquantile\u001b[0;34m(self, q, axis, numeric_only, interpolation)\u001b[0m\n\u001b[1;32m   8294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8295\u001b[0m         result = data._data.quantile(\n\u001b[0;32m-> 8296\u001b[0;31m             \u001b[0mqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_transposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8297\u001b[0m         )\n\u001b[1;32m   8298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mquantile\u001b[0;34m(self, axis, consolidate, transposed, interpolation, qs, numeric_only)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0maxe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_axe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mquantile\u001b[0;34m(self, qs, interpolation, axis)\u001b[0m\n\u001b[1;32m   1554\u001b[0m             \u001b[0;31m# Note: we use self.values below instead of values because the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m             \u001b[0;31m#  `asi8` conversion above will behave differently under `isna`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1556\u001b[0;31m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1557\u001b[0m             result = nanpercentile(\n\u001b[1;32m   1558\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/dtypes/missing.py\u001b[0m in \u001b[0;36misna\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \"\"\"\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_isna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/dtypes/missing.py\u001b[0m in \u001b[0;36m_isna_new\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    145\u001b[0m         ),\n\u001b[1;32m    146\u001b[0m     ):\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_isna_ndarraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCGeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/dtypes/missing.py\u001b[0m in \u001b[0;36m_isna_ndarraylike\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0miNaT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;31m# box\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "monitor(train_data, test_data.head(1000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# EXPLORATION CELLS FOLLOWING:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['fare-bin'].value_counts().sort_index().plot.bar(color = 'b', edgecolor = 'k');\n",
    "plt.title('Fare Binned');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

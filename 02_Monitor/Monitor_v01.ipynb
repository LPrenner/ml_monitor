{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed \n",
    "RSEED = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training Data\n",
    "\n",
    "train_data = pd.read_csv(\"../00_Data_Sets/1_mio_dataset_2010_2014.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Input Data\n",
    "incoming_data_passenger_count_outlier = pd.read_csv(\"../01_Synthetic Data/02_Passenger Count/passenger count_outlier_01.csv\");\n",
    "# incoming_data_location_outlier = pd.read_csv(\"../01_Synthetic Data/01_Location/location_outlier_01.csv\");\n",
    "# incoming_data_distance_outlier = pd.read_csv(\"../01_Synthetic Data/03_Distance/distance_outlier_01.csv\");\n",
    "# incoming_data_distance_drift = pd.read_csv(\"../01_Synthetic Data/03_Distance/distance_drift_total_01.csv\");\n",
    "# incoming_data_location_drift = pd.read_csv(\"../01_Synthetic Data/01_Location/location_drift_total.csv\");\n",
    "incoming_data_passenger_drift = pd.read_csv(\"../01_Synthetic Data/02_Passenger Count/passenger count_drift_total_01.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_drift_main_metrics(training_data, incoming_batch, start_index, end_index, batch_info, column, thresholds):\n",
    "    \n",
    "    batch_thresholds = thresholds[batch_info['name'].lower() + '_batch_drift']\n",
    "    threshold = batch_thresholds['one_dim_drift_metric']\n",
    "            \n",
    "    if abs(incoming_batch.iloc[start_index:end_index,column].mean()) > abs((training_data.iloc[:,column].mean() * (1 + threshold))):\n",
    "        print('[DRIFT][{} Window]: Upwards Data Drift detected!. ({} AVG: {})\\t(Index: {})'.format(batch_info['name'], training_data.columns[column], incoming_batch.iloc[start_index:end_index,column].mean(), end_index))\n",
    "        \n",
    "    if abs(incoming_batch.iloc[start_index:end_index,column].quantile(0.25)) > abs((training_data.iloc[:,column].quantile(0.25) * (1 + threshold))):\n",
    "        print('[DRIFT][{} Window]: Upwards Data Drift detected! ({} 25%-Quantile: {})\\t(Index: {})'.format(batch_info['name'], training_data.columns[column], incoming_batch.iloc[start_index:end_index,column].mean(), end_index))\n",
    "        \n",
    "    if abs(incoming_batch.iloc[start_index:end_index,column].quantile(0.75)) > abs((training_data.iloc[:,column].quantile(0.75) * (1 + threshold))):\n",
    "        print('[DRIFT][{} Window]: Upwards Data Drift detected! ({} 75%-Quantile: {})\\t(Index: {})'.format(batch_info['name'], training_data.columns[column], incoming_batch.iloc[start_index:end_index,column].mean(), end_index))\n",
    "        \n",
    "    if abs(incoming_batch.iloc[start_index:end_index,column].mean()) < abs((training_data.iloc[:,column].mean() * (1 - threshold))):\n",
    "        print('[DRIFT][{} Window]: Downwards Data Drift detected! ({} AVG: {})\\t(Index: {})'.format(batch_info['name'], training_data.columns[column], incoming_batch.iloc[start_index:end_index,column].mean(), end_index))\n",
    "        \n",
    "    if abs(incoming_batch.iloc[start_index:end_index,column].quantile(0.25)) < abs((training_data.iloc[:,column].quantile(0.25) * (1 - threshold))):\n",
    "        print('[DRIFT][{} Window]: Downwards Data Drift detected! ({} 25%-Quantile: {})\\t(Index: {})'.format(batch_info['name'], training_data.columns[column], incoming_batch.iloc[start_index:end_index,column].mean(), end_index))\n",
    "        \n",
    "    if abs(incoming_batch.iloc[start_index:end_index,column].quantile(0.75)) < abs((training_data.iloc[:,column].quantile(0.75) * (1 - threshold))):\n",
    "        print('[DRIFT][{} Window]: Downwards Data Drift detected! ({} 75%-Quantile: {})\\t(Index: {})'.format(batch_info['name'], training_data.columns[column], incoming_batch.iloc[start_index:end_index,column].mean(), end_index))\n",
    "\n",
    "        \n",
    "from math import log2\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    "\n",
    "        \n",
    "def monitor_drift_kl_divergence(training_data, incoming_batch, start_index, end_index, batch_info, column, thresholds):\n",
    "    \n",
    "    batch_thresholds = thresholds[batch_info['name'].lower() + '_batch_drift']\n",
    "    threshold = batch_thresholds['one_dim_drift_kl_divergence']\n",
    "    \n",
    "    # Filter out zero values in datasets\n",
    "    batch_df = incoming_batch[incoming_batch.iloc[:,column] != 0]\n",
    "    train_df = training_data[training_data.iloc[:,column] != 0]\n",
    "    \n",
    "    # Filter out negative values in dataset, if mean is positive (= remove negative outliers)\n",
    "    if batch_df.iloc[:,column].mean() > 0:\n",
    "        batch_df = batch_df[batch_df.iloc[:,column] > 0]\n",
    "        \n",
    "    batch_df = batch_df.reset_index()\n",
    "    train_df = train_df.reset_index()\n",
    "    batch_df = batch_df.drop('index', axis=1)\n",
    "    train_df = train_df.drop('index', axis=1)\n",
    "    batch = batch_df.iloc[start_index:end_index,column]\n",
    "    batch = batch.reset_index()\n",
    "    batch = batch.drop('index', axis=1)\n",
    "    batch = batch.iloc[:,0]\n",
    "    train = train_df.iloc[:,column]\n",
    "    \n",
    "    divergence_score = kl_divergence(batch, train);\n",
    "    \n",
    "    if divergence_score > threshold:\n",
    "        print('[DRIFT][{} Window]: KL-Divergence detected! ({} Divergence: {})\\t(Index: {})'.format(batch_info['name'], batch_df.columns[column], divergence_score, end_index))        \n",
    "        \n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "def monitor_drift_wasserstein_distance(training_data, incoming_batch, start_index, end_index, batch_info, column, thresholds):\n",
    "    \n",
    "    batch_thresholds = thresholds[batch_info['name'].lower() + '_batch_drift']\n",
    "    threshold = batch_thresholds['one_dim_drift_wasserstein']\n",
    "    \n",
    "    batch = incoming_batch.iloc[start_index:end_index, column]\n",
    "    train = training_data.iloc[:, column]\n",
    "    wasserstein_dist = wasserstein_distance(batch, train);\n",
    "    \n",
    "    if wasserstein_dist > threshold:\n",
    "        print('[DRIFT][{} Window]: Great Wasserstein Distance detected! ({} Distance: {})\\t(Index: {})'.format(batch_info['name'], incoming_batch.columns[column], wasserstein_dist, end_index))\n",
    "                    \n",
    "def monitor_drift_one_dimension(training_data, train_data_infos, incoming_batch, start_index, end_index, batch_info, thresholds):\n",
    "    \n",
    "    columns_to_use = train_data_infos['columns_to_use']\n",
    "    \n",
    "    for column in columns_to_use:\n",
    "        monitor_drift_main_metrics(training_data, incoming_batch, start_index, end_index, batch_info, column, thresholds);\n",
    "        monitor_drift_kl_divergence(training_data, incoming_batch, start_index, end_index, batch_info, column, thresholds);\n",
    "        monitor_drift_wasserstein_distance(training_data, incoming_batch, start_index, end_index, batch_info, column, thresholds);\n",
    "    \n",
    "def monitor_drift_multi_dimensions(training_data, train_data_infos, incoming_batch, start_index, end_index, batch_info, thresholds):\n",
    "    columns_to_use = train_data_infos['columns_to_use'];\n",
    "    pca_model = train_data_infos['pca_model'];\n",
    "    \n",
    "    incoming_batch_filtered = get_data_filtered(incoming_batch, columns_to_use);\n",
    "    incoming_batch_transformed = transform_pca(pca_model, incoming_batch_filtered);\n",
    "    distances = calculate_euclidean_distances(incoming_batch_transformed, columns_to_use);\n",
    "    distances_metrics = get_distance_metrics(distances, columns_to_use, pca_model, thresholds);\n",
    "    \n",
    "    if distances_metrics['mean'] > train_data_infos['mean']:\n",
    "        print('[DRIFT] Multi-Dimensional Drift Detected! Mean Distance: {}\\t\\t\\t(Index: {})'.format(distances_metrics['maximum'], end_index));\n",
    "        \n",
    "    if distances_metrics['first_quarter'] > train_data_infos['first_quarter']:\n",
    "        print('[DRIFT] Multi-Dimensional Drift Detected! Distance: {}\\t\\t\\t(Index: {})'.format(distances_metrics['first_quarter'], end_index));\n",
    "        \n",
    "    if distances_metrics['third_quarter'] > train_data_infos['third_quarter']:\n",
    "        print('[DRIFT] Multi-Dimensional Drift Detected! Distance: {}\\t\\t\\t(Index: {})'.format(distances_metrics['third_quarter'], end_index));\n",
    "        \n",
    "        \n",
    "def monitor_batch_drift(training_data, train_data_infos, incoming_batch, index, batch_info, thresholds):\n",
    "    \n",
    "    batch_size = batch_info['size'];\n",
    "    start_index = index - batch_size + 1;\n",
    "    end_index = index;\n",
    "    \n",
    "    monitor_drift_one_dimension(training_data, train_data_infos, incoming_batch, start_index, end_index, batch_info, thresholds);\n",
    "    monitor_drift_multi_dimensions(training_data, train_data_infos, incoming_batch, start_index, end_index, batch_info, thresholds);\n",
    "    \n",
    "def monitor_drift(training_data, train_data_infos, batch, step_sizes, index, row, batch_infos, thresholds):\n",
    "    \n",
    "    # Add new sample to batch\n",
    "    batch.loc[index] = row.values;\n",
    "        \n",
    "    if batch.shape[0] % step_sizes['small'] == 0 and batch.shape[0] > (step_sizes['small']):\n",
    "        monitor_batch_drift(training_data, train_data_infos, batch, index, batch_infos['small'], thresholds);\n",
    "\n",
    "    if batch.shape[0] % step_sizes['medium'] == 0:\n",
    "        monitor_batch_drift(training_data, train_data_infos, batch, index, batch_infos['medium'], thresholds);\n",
    "\n",
    "    if batch.shape[0] % step_sizes['large'] == 0:\n",
    "        monitor_batch_drift(training_data, train_data_infos, batch, index, batch_infos['large'], thresholds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_samples_per_day(data):\n",
    "    # TODO: More elegant solution possible?\n",
    "    df = pd.DataFrame();\n",
    "    df['pickup_datetime'] = train_data['pickup_datetime']\n",
    "    df['count'] = 1\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df.index = df['pickup_datetime'] \n",
    "    df = df.resample('D').sum()\n",
    "    df = df[df['count'] != 0]\n",
    "    return df['count'].mean()\n",
    "\n",
    "def get_batch_infos(data):\n",
    "    batch_size_day = get_avg_samples_per_day(data);\n",
    "    batch_size_week = batch_size_day * 7;\n",
    "    batch_size_month = batch_size_day * 30;\n",
    "    \n",
    "    batch_sizes = {\n",
    "        'small': {'size': int(batch_size_day), 'name': 'Small'},\n",
    "        'medium': {'size': int(batch_size_week), 'name': 'Medium'},\n",
    "        'large': {'size': int(batch_size_month), 'name': 'Large'},        \n",
    "    }\n",
    "    \n",
    "    return batch_sizes;\n",
    "\n",
    "def get_step_sizes(batch_infos):\n",
    "    \n",
    "    step_sizes = {\n",
    "        'small': (batch_infos['small']['size'] / 2),\n",
    "        'medium': (batch_infos['medium']['size'] / 2),\n",
    "        'large': (batch_infos['large']['size'] / 2) \n",
    "    }\n",
    "    return step_sizes;\n",
    "\n",
    "def get_data_filtered(data, columns_to_use):\n",
    "    data_filtered = pd.DataFrame()\n",
    "    i = 0\n",
    "\n",
    "    # Filter Input Columns\n",
    "    for column in columns_to_use:\n",
    "        data_filtered.insert(i, data.columns[column], data.iloc[:,column])\n",
    "        i = i + 1\n",
    "        \n",
    "    return data_filtered;\n",
    "\n",
    "def apply_pca(train_data_filtered):\n",
    "    pca_model = get_pca_model(train_data_filtered);\n",
    "    return pca_model, transform_pca(pca_model, train_data_filtered);\n",
    "\n",
    "def get_pca_model(train_data_filtered):\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(whiten=True)\n",
    "    return pca.fit(train_data_filtered)\n",
    "\n",
    "def transform_pca(pca_model, train_data_filtered):\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    return pca_model.transform(train_data_filtered)\n",
    "\n",
    "def get_zero_vector(columns_to_use):\n",
    "    number_of_dimensions = len(columns_to_use)\n",
    "    return np.zeros((number_of_dimensions)*1)\n",
    "\n",
    "def calculate_euclidean_distance(sample, zero_vector):\n",
    "    from scipy.spatial import distance\n",
    "    \n",
    "    return distance.euclidean(sample, zero_vector)\n",
    "\n",
    "def calculate_euclidean_distances(transformed_data, columns_to_use):\n",
    "    from scipy.spatial import distance\n",
    "    \n",
    "    zero_vector = get_zero_vector(columns_to_use);\n",
    "    \n",
    "    index = 0\n",
    "    distances = np.ndarray(shape=(np.size(transformed_data,0),1))\n",
    "\n",
    "    # Create distances to zero vector\n",
    "    for sample in transformed_data:\n",
    "        distances[index] = calculate_euclidean_distance(transformed_data[index], zero_vector);\n",
    "        index = index + 1\n",
    "        \n",
    "    return distances;\n",
    "\n",
    "def get_distance_metrics(distances, columns_to_use, pca_model, thresholds):\n",
    "    \n",
    "    raw_threshold = thresholds['multi_dim_outlier']\n",
    "    threshold = raw_threshold * 100\n",
    "    \n",
    "    train_data_infos = {\n",
    "        'columns_to_use': columns_to_use,\n",
    "        'pca_model': pca_model,\n",
    "        'minimum': distances.min(),\n",
    "        'lower_threshold_percentile': np.percentile(distances, threshold),\n",
    "        'first_quarter': np.percentile(distances, 25),\n",
    "        'mean': distances.mean(),\n",
    "        'third_quarter': np.percentile(distances, 75),\n",
    "        'upper_threshold_percentile': np.percentile(distances, (100 - threshold)),\n",
    "        'maximum': distances.max()\n",
    "    }\n",
    "    \n",
    "    return train_data_infos;\n",
    "\n",
    "def get_train_data_infos(train_data, columns_to_use, thresholds):\n",
    "    \n",
    "    # +++ REMOVE: Only for testing purposes +++\n",
    "    test = {\n",
    "        'columns_to_use': columns_to_use,\n",
    "        'pca_model': get_pca_model(get_data_filtered(train_data, columns_to_use)),\n",
    "        'minimum': 0.7249817600534729, \n",
    "        'lower_threshold_percentile': 1.0764902427267176,\n",
    "        'first_quarter': 1.1133579226848411,\n",
    "        'mean': 1.922411839793231,\n",
    "        'third_quarter': 2.200820905397247,\n",
    "        'upper_threshold_percentile': 8.095414750208105, \n",
    "        'maximum': 58.320576749831\n",
    "    }\n",
    "    \n",
    "    # return test\n",
    "    # +++ REMOVE: Only for testing purposes +++\n",
    "    \n",
    "    train_data_filtered = get_data_filtered(train_data, columns_to_use);\n",
    "    pca_model, transformed_data = apply_pca(train_data_filtered)\n",
    "    distances = calculate_euclidean_distances(transformed_data, columns_to_use)\n",
    "    \n",
    "    return get_distance_metrics(distances, columns_to_use, pca_model, thresholds);  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_outliers_one_dimension(train_data, train_data_infos, incoming_sample, index, thresholds):\n",
    "    \n",
    "    threshold = thresholds['one_dim_outlier']\n",
    "    columns_to_use = train_data_infos['columns_to_use']\n",
    "    \n",
    "    for column in columns_to_use:\n",
    "            \n",
    "            if incoming_sample[column] > train_data.iloc[:,column].max():\n",
    "                print('[OUTLIER] MAX Outlier Detected! {}: {}\\t\\t\\t(Index: {})'.format(train_data.columns[column], incoming_sample[column], index));\n",
    "\n",
    "            elif incoming_sample[column] > train_data.iloc[:,column].quantile(1 - threshold):\n",
    "                print('[POT. OUTLIER] Potential MAX Outlier Detected! {}: {}\\t\\t\\t(Index: {})'.format(train_data.columns[column], incoming_sample[column], index));\n",
    "\n",
    "            if incoming_sample[column] < train_data.iloc[:,column].min():\n",
    "                print('[OUTLIER] MIN Outlier Detected! {}: {}\\t\\t\\t(Index: {})'.format(train_data.columns[column], incoming_sample[column], index));\n",
    "                \n",
    "            elif incoming_sample[column] < train_data.iloc[:,column].quantile(threshold):\n",
    "                print('[POT. OUTLIER] Potential MIN Outlier Detected! {}: {}\\t\\t\\t(Index: {})'.format(train_data.columns[column], incoming_sample[column], index));\n",
    "                \n",
    "    \n",
    "def monitor_outliers_multi_dimensions(train_data, train_data_infos, incoming_sample, index):\n",
    "    incoming_sample_df = pd.DataFrame(incoming_sample).transpose();\n",
    "    incoming_sample_df_filtered = get_data_filtered(incoming_sample_df, train_data_infos['columns_to_use']);\n",
    "    incoming_sample_df_transformed = transform_pca(train_data_infos['pca_model'], incoming_sample_df_filtered);\n",
    "    distance = calculate_euclidean_distance(incoming_sample_df_transformed, get_zero_vector(train_data_infos['columns_to_use']));\n",
    "\n",
    "    if distance > train_data_infos['maximum']:\n",
    "        print('[OUTLIER] Multi-Dimensional Outlier Detected! Distance: {}\\t\\t\\t(Index: {})'.format(distance, index));\n",
    "        \n",
    "    elif distance > train_data_infos['upper_threshold_percentile']:\n",
    "        print('[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: {}\\t\\t\\t(Index: {})'.format(distance, index));\n",
    "\n",
    "def monitor_outliers(train_data, train_data_infos, incoming_sample, index, thresholds):\n",
    "    monitor_outliers_one_dimension(train_data, train_data_infos, incoming_sample, index, thresholds);\n",
    "    monitor_outliers_multi_dimensions(train_data, train_data_infos, incoming_sample, index);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define desired thresholds\n",
    "\n",
    "def get_thresholds():\n",
    "    thresholds = {\n",
    "        'one_dim_outlier': 0.001,\n",
    "        'multi_dim_outlier': 0.01,\n",
    "        'small_batch_drift': {\n",
    "            'one_dim_drift_metric': 0.03,\n",
    "            'one_dim_drift_kl_divergence': 100,\n",
    "            'one_dim_drift_wasserstein': 0.02\n",
    "        },\n",
    "        'medium_batch_drift': {\n",
    "            'one_dim_drift_metric': 0.05,\n",
    "            'one_dim_drift_kl_divergence': 150,\n",
    "            'one_dim_drift_wasserstein': 0.03\n",
    "        },\n",
    "        'large_batch_drift': {\n",
    "            'one_dim_drift_metric': 0.07,\n",
    "            'one_dim_drift_kl_divergence': 200,\n",
    "            'one_dim_drift_wasserstein': 0.04\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return thresholds;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor(train_data, incoming_data):\n",
    "        \n",
    "    thresholds = get_thresholds();\n",
    "    columns_to_use = [3, 4, 5, 6, 7, 12, 13, 14]\n",
    "    batch = pd.DataFrame(columns=incoming_data.columns);\n",
    "    batch_infos = get_batch_infos(train_data);\n",
    "    train_data_infos = get_train_data_infos(train_data, columns_to_use, thresholds);\n",
    "    step_sizes = get_step_sizes(batch_infos);\n",
    "    \n",
    "    for index, row in incoming_data.iterrows():\n",
    "        \n",
    "        monitor_outliers(train_data, train_data_infos, row, index, thresholds);\n",
    "        monitor_drift(train_data, train_data_infos, batch, step_sizes, index, row, batch_infos, thresholds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[POT. OUTLIER] Potential MAX Outlier Detected! dropoff_latitude: 40.887882232666016\t\t\t(Index: 4)\n",
      "[OUTLIER] MIN Outlier Detected! passenger_count: -1\t\t\t(Index: 150)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! manhattan: 0.36305999755859375\t\t\t(Index: 343)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! euclidean: 0.2629007645951818\t\t\t(Index: 343)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! haversine: 27.40050231658701\t\t\t(Index: 343)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 12.049343616294522\t\t\t(Index: 343)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 10.145906069419583\t\t\t(Index: 344)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! pickup_latitude: 40.86755752563477\t\t\t(Index: 424)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! manhattan: 0.3475189208984233\t\t\t(Index: 440)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! euclidean: 0.2458510235458707\t\t\t(Index: 440)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! haversine: 24.076034903779558\t\t\t(Index: 440)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 9.791922406193613\t\t\t(Index: 440)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 9.677755204164434\t\t\t(Index: 444)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 9.879007906230157\t\t\t(Index: 581)\n",
      "[DRIFT][Small Window]: Downwards Data Drift detected! (passenger_count AVG: 1.5238879736408566)\t(Index: 607)\n",
      "[DRIFT][Small Window]: KL-Divergence detected! (passenger_count Divergence: 272.05312581281686)\t(Index: 607)\n",
      "[DRIFT][Small Window]: Great Wasserstein Distance detected! (passenger_count Distance: 0.0568685998302078)\t(Index: 607)\n",
      "[DRIFT][Small Window]: Upwards Data Drift detected! (manhattan 25%-Quantile: 0.04520096378232211)\t(Index: 607)\n",
      "[DRIFT][Small Window]: Upwards Data Drift detected! (manhattan 75%-Quantile: 0.04520096378232211)\t(Index: 607)\n",
      "[DRIFT][Small Window]: Upwards Data Drift detected! (euclidean 75%-Quantile: 0.0346845378111404)\t(Index: 607)\n",
      "[DRIFT][Small Window]: Upwards Data Drift detected! (haversine 25%-Quantile: 3.416607411665881)\t(Index: 607)\n",
      "[DRIFT][Small Window]: Upwards Data Drift detected! (haversine 75%-Quantile: 3.416607411665881)\t(Index: 607)\n",
      "[DRIFT][Small Window]: KL-Divergence detected! (haversine Divergence: 2347.004743930401)\t(Index: 607)\n",
      "[DRIFT][Small Window]: Great Wasserstein Distance detected! (haversine Distance: 0.2534633917892972)\t(Index: 607)\n",
      "[DRIFT] Multi-Dimensional Drift Detected! Distance: 1.2605423068634236\t\t\t(Index: 607)\n",
      "[DRIFT] Multi-Dimensional Drift Detected! Distance: 2.5070246716793303\t\t\t(Index: 607)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 11.937218647779211\t\t\t(Index: 621)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! dropoff_longitude: -73.62210845947266\t\t\t(Index: 631)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! dropoff_latitude: 41.02389526367188\t\t\t(Index: 631)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! manhattan: 0.495525360107429\t\t\t(Index: 631)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! euclidean: 0.3504990597107201\t\t\t(Index: 631)\n",
      "[POT. OUTLIER] Potential MAX Outlier Detected! haversine: 34.81891090248362\t\t\t(Index: 631)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 16.130677218461617\t\t\t(Index: 631)\n",
      "[POT. OUTLIER] Potential Multi-Dimensional Outlier Detected! Distance: 9.335628450471807\t\t\t(Index: 666)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d46d08c1ee29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincoming_data_passenger_count_outlier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-07971bd15ab9>\u001b[0m in \u001b[0;36mmonitor\u001b[0;34m(train_data, incoming_data)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmonitor_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmonitor_drift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-5a8594e80f20>\u001b[0m in \u001b[0;36mmonitor_drift\u001b[0;34m(training_data, train_data_infos, batch, step_sizes, index, row, batch_infos, thresholds)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstep_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'small'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'small'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mmonitor_batch_drift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'small'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstep_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'medium'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5a8594e80f20>\u001b[0m in \u001b[0;36mmonitor_batch_drift\u001b[0;34m(training_data, train_data_infos, incoming_batch, index, batch_info, thresholds)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mend_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mmonitor_drift_one_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincoming_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mmonitor_drift_multi_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincoming_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5a8594e80f20>\u001b[0m in \u001b[0;36mmonitor_drift_one_dimension\u001b[0;34m(training_data, train_data_infos, incoming_batch, start_index, end_index, batch_info, thresholds)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns_to_use\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mmonitor_drift_main_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincoming_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mmonitor_drift_kl_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincoming_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mmonitor_drift_wasserstein_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincoming_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5a8594e80f20>\u001b[0m in \u001b[0;36mmonitor_drift_kl_divergence\u001b[0;34m(training_data, incoming_batch, start_index, end_index, batch_info, column, thresholds)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Filter out zero values in datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mbatch_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincoming_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mincoming_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Filter out negative values in dataset, if mean is positive (= remove negative outliers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2984\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3037\u001b[0m         \u001b[0;31m# be reindexed to match DataFrame rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3038\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3039\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3040\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "monitor(train_data, incoming_data_passenger_count_outlier.head(1000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
